import json
import numpy as np
import trimesh
import torch
from torchvision.utils import save_image
import os
import math
import scipy.io

# ğŸ”§ PyTorch3D ë Œë”ë§ êµ¬ì„±``
from pytorch3d.structures import Meshes
from pytorch3d.renderer import (
    FoVPerspectiveCameras, FoVOrthographicCameras, RasterizationSettings,
    MeshRenderer, MeshRasterizer, SoftPhongShader,
    PointLights, TexturesVertex
)

def euler_to_rot_matrix(angle):
    """ì˜¤ì¼ëŸ¬ ê° (yaw, pitch, roll)ì„ íšŒì „ í–‰ë ¬ë¡œ ë³€í™˜"""
    yaw, pitch, roll = angle[0], angle[1], angle[2]

    Rx = torch.tensor([
        [1, 0, 0],
        [0, math.cos(roll), -math.sin(roll)],
        [0, math.sin(roll),  math.cos(roll)],
    ], dtype=torch.float32)

    Ry = torch.tensor([
        [ math.cos(pitch), 0, math.sin(pitch)],
        [0, 1, 0],
        [-math.sin(pitch), 0, math.cos(pitch)],
    ], dtype=torch.float32)

    Rz = torch.tensor([
        [math.cos(yaw), -math.sin(yaw), 0],
        [math.sin(yaw),  math.cos(yaw), 0],
        [0, 0, 1],
    ], dtype=torch.float32)

    R = Rz @ Ry @ Rx
    R = R.permute(0, 1)
    return R

def intrinsics_to_fov(intrinsics, image_size):
    #fx = intrinsics[0, 0]
    #fy = intrinsics[1, 1]
    fx = 1015
    fy = 1015
    w, h = image_size, image_size  # ì •ì‚¬ê°í˜• ê¸°ì¤€

    fov_x = 2 * math.atan(112 / (fx)) * 180 / math.pi
    fov_y = 2 * math.atan(112 / (fy)) * 180 / math.pi
    #fov_x = 2 * math.atan(w / (2 * fx)) * 180 / math.pi
    #fov_y = 2 * math.atan(h / (2 * fy)) * 180 / math.pi
    return fov_x, fov_y

def render_mesh_image(vertices, faces, c2w_matrix, image_size=512, device="cuda"):
    """
    ë Œë”ë§ í•¨ìˆ˜: ë©”ì‰¬ì™€ camera-to-world poseë¡œ ì •ì§€ ì´ë¯¸ì§€ ë Œë”ë§`
    """
    # 1. ë©”ì‰¬ ì„¤ì •

    face_model_path = '/source/Hyeonho/Research/MonoFace/submodules/eg3d/dataset_preprocessing/ffhq/Deep3DFaceRecon_pytorch/BFM/BFM_model_front.mat'
    mat_path = "/source/Hyeonho/Research/MonoFace/submodules/eg3d/dataset_preprocessing/ffhq/Deep3DFaceRecon_pytorch/checkpoints/pretrained/results/input_image/epoch_20_000000/39798.mat"
    data = scipy.io.loadmat(mat_path)
    angle = data['angle'].squeeze()
    print(angle)
    trans = data['trans'].squeeze()
    print(trans)
    print(data)

    face_model = scipy.io.loadmat(face_model_path)
    print(face_model.keys())


    #angle = [0.09270443, -0.03940581, 0.00396501]
    #angle = [-0.0528, -0.0843,  0.0154]
    #trans = [0.00373354, -0.0542998, 0.25858536]

    # ì •ì  ì„¤ì • ë° ë³€í™˜
    verts = torch.tensor(vertices, dtype=torch.float32, device=device)
    R = euler_to_rot_matrix(angle).to(device)
    T = torch.tensor(trans, dtype=torch.float32, device=device)

    # ì •ì ì— íšŒì „ ë° ì´ë™ ì ìš©
    verts = verts @ R + T.unsqueeze(0)

    #verts = torch.tensor(vertices, dtype=torch.float32, device=device)
    #verts = verts + torch.tensor(([ 0.00373354, -0.0542998 , 0.25858536]), dtype=torch.float32, device=device).unsqueeze(0) 
    faces = torch.tensor(faces, dtype=torch.int64, device=device)
    verts_rgb = torch.ones_like(verts)[None]  # í°ìƒ‰ í…ìŠ¤ì²˜
    textures = TexturesVertex(verts_features=verts_rgb)
    mesh = Meshes(verts=[verts], faces=[faces], textures=textures)

    # 2. camera-to-world â†’ world-to-camera ë³€í™˜
    flip = np.diag([1, -1, 1, 1])
    w2c = flip @ np.linalg.inv(c2w_matrix)
    print(w2c)
    R = torch.tensor(w2c[:3, :3][None], dtype=torch.float32, device=device)  # (1, 3, 3)
    T = torch.tensor(w2c[:3, 3][None], dtype=torch.float32, device=device)   # (1, 3)

    # 3. ì¹´ë©”ë¼ ì •ì˜
    cameras = FoVPerspectiveCameras(R=R, T=T, device=device)
    print(cameras)

    # 4. ë Œë”ëŸ¬ êµ¬ì„±
    raster_settings = RasterizationSettings(
        image_size=image_size,
        blur_radius=0.0,
        faces_per_pixel=1,
    )
    lights = PointLights(device=device, location=[[0.0, 0.0, 3.0]])
    
    renderer = MeshRenderer(
        rasterizer=MeshRasterizer(cameras=cameras, raster_settings=raster_settings),
        shader=SoftPhongShader(device=device, cameras=cameras, lights=lights)
    )

    # 5. ë Œë”ë§
    images = renderer(mesh)  # (1, H, W, 3)
    image = images[0, ..., :3].permute(2, 0, 1)  # (3, H, W)
    return image

def main():
    # ğŸ”§ ê²½ë¡œ ì„¤ì •
    json_path = "/source/Hyeonho/Research/MonoFace/submodules/Diffportrait360/diffportrait360_release/sample_data/input_image/dataset.json"
    mesh_path = "/source/Hyeonho/Research/MonoFace/submodules/eg3d/dataset_preprocessing/ffhq/Deep3DFaceRecon_pytorch/checkpoints/pretrained/results/input_image/epoch_20_000000/39798.obj"        # ë Œë”ë§í•  .obj ë©”ì‰¬ ê²½ë¡œ
    #mesh_path = '/source/Hyeonho/Research/MonoFace/data/bfm.obj'
    output_path = "rendered.png" # ì¶œë ¥ ì´ë¯¸ì§€ ê²½ë¡œ
    label_index = 0                    # dataset.jsonì—ì„œ ì‚¬ìš©í•  label index

    device = "cuda" if torch.cuda.is_available() else "cpu"

    # 1. JSON íŒŒì¼ ë¡œë”©
    with open(json_path, "r") as f:
        data = json.load(f)

    # 2. 25D ì¹´ë©”ë¼ íŒŒë¼ë¯¸í„° ì¶”ì¶œ
    camera_vec = np.array([float(x) for x in data["labels"][label_index][1]], dtype=np.float32)
    c2w = camera_vec[:16].reshape(4, 4)  # camera-to-world
    intrinsics = camera_vec[16:].reshape(3, 3)  # (ì„ íƒì‚¬í•­)
    print("intrinsics: ", intrinsics)
    
    fov_x, fov_y = intrinsics_to_fov(intrinsics, image_size=1024)
    print(f"fov_x: {fov_x:.2f} degrees")
    print(f"fov_y: {fov_y:.2f} degrees")

    # 3. ë©”ì‰¬ ë¡œë”©
    mesh = trimesh.load(mesh_path, process=False)
    vertices = mesh.vertices
    faces = mesh.faces

    # 4. ë Œë”ë§
    image = render_mesh_image(vertices, faces, c2w_matrix=c2w, image_size=1024, device=device)

    # 5. ì €ì¥
    save_image(image, output_path)
    print(f"âœ… ë Œë”ë§ ì™„ë£Œ â†’ {output_path}")

if __name__ == "__main__":
    main()
